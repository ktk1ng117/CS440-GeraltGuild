{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kingk\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java train size: 6091, Java val size: 1523\n",
      "Python train size: 1507, Python val size: 377\n",
      "Pharo train size: 1038, Pharo val size: 260\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.classification import MultilabelPrecision, MultilabelRecall, MultilabelF1Score\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "\n",
    "\n",
    "splits = {'java_train': 'data/java_train-00000-of-00001.parquet', 'java_test': 'data/java_test-00000-of-00001.parquet', 'python_train': 'data/python_train-00000-of-00001.parquet', 'python_test': 'data/python_test-00000-of-00001.parquet', 'pharo_train': 'data/pharo_train-00000-of-00001.parquet', 'pharo_test': 'data/pharo_test-00000-of-00001.parquet'}\n",
    "\n",
    "java_train = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"java_train\"])\n",
    "java_test = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"java_test\"])\n",
    "\n",
    "python_train = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"python_train\"])\n",
    "python_test = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"python_test\"])\n",
    "\n",
    "pharo_train = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"pharo_train\"])\n",
    "pharo_test = pd.read_parquet(\"hf://datasets/NLBSE/nlbse25-code-comment-classification/\" + splits[\"pharo_test\"])\n",
    "\n",
    "# Split Java dataset\n",
    "java_train_data, java_val_data = train_test_split(java_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split Python dataset\n",
    "python_train_data, python_val_data = train_test_split(python_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split Pharo dataset\n",
    "pharo_train_data, pharo_val_data = train_test_split(pharo_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Java train size: {len(java_train_data)}, Java val size: {len(java_val_data)}\")\n",
    "print(f\"Python train size: {len(python_train_data)}, Python val size: {len(python_val_data)}\")\n",
    "print(f\"Pharo train size: {len(pharo_train_data)}, Pharo val size: {len(pharo_val_data)}\")\n",
    "\n",
    "#print(java_train.iloc[0, :])\n",
    "#print(python_train.iloc[0, :])\n",
    "#print(pharo_train.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JavaCommentDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.comments = dataframe['combo'].tolist()\n",
    "        self.labels = dataframe['labels'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the text\n",
    "        text = self.comments[idx]\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Process labels\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        \n",
    "        # Reshape input for CNN\n",
    "        input_ids = tokens['input_ids'].squeeze(0)\n",
    "        \n",
    "        # Reshape embeddings to match CNN input format [batch_size, channels, sequence_length, embedding_dim]\n",
    "        cnn_input = input_ids.unsqueeze(0)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': cnn_input,\n",
    "            'labels': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "max_len = 512 \n",
    "\n",
    "# Prepare Dataset\n",
    "train_dataset = JavaCommentDataset(java_train_data, tokenizer, max_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = JavaCommentDataset(java_val_data, tokenizer, max_len)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataset = JavaCommentDataset(java_test, tokenizer, max_len)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Metric to monitor\n",
    "    mode=\"min\",          # Use \"max\" if monitoring accuracy\n",
    "    patience=10,          # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=True         # Prints information about early stopping when triggered\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(PyTorchCNN, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "        # CNN layers definition\n",
    "        self.cnn_layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 3, kernel_size=(5, embed_dim)),  # Example embedding size\n",
    "            torch.nn.BatchNorm2d(3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=(2, 1)), \n",
    "            \n",
    "            torch.nn.Conv2d(3, 16, kernel_size=(3, 1)),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=(2, 1)), \n",
    "            \n",
    "            torch.nn.Conv2d(16, 32, kernel_size=(3, 1)),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=(2, 1))\n",
    "        )\n",
    "        # Dynamically calculate flattened size\n",
    "        self.flattened_size = self._get_flattened_size(embed_dim)\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.flattened_size, 512),\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            torch.nn.Linear(512, 256),                  # Second layer\n",
    "            torch.nn.BatchNorm1d(256),                  \n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            torch.nn.Linear(256, 128),                  # Third layer\n",
    "            torch.nn.BatchNorm1d(128),                  \n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            torch.nn.Linear(128, num_classes)  # Output layer\n",
    "        )\n",
    "    def _get_flattened_size(self, embed_dim):\n",
    "        \"\"\"\n",
    "        Computes the size of the flattened output after the CNN layers.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Create a dummy input of shape [batch_size=1, channels=1, sequence_length, embed_dim]\n",
    "            dummy_input = torch.zeros(1, 1, 512, embed_dim)\n",
    "            cnn_out = self.cnn_layers(dummy_input)\n",
    "            return cnn_out.numel()\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embedding(input_ids)  # Shape: [batch_size, sequence_length, embed_dim]\n",
    "\n",
    "        #Pass through CNN layers\n",
    "        cnn_out = self.cnn_layers(embeddings) \n",
    "\n",
    "        cnn_out = torch.flatten(cnn_out, 1) \n",
    "\n",
    "        output = self.fc_layers(cnn_out) \n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.num_classes = num_classes\n",
    "        # Metrics\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multilabel\", num_labels=num_classes)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multilabel\", num_labels=num_classes)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multilabel\", num_labels=num_classes)\n",
    "\n",
    "        # Initialize class-wise accuracy tracking\n",
    "        self.class_wise_train_acc = {i: torchmetrics.Accuracy(task=\"multilabel\", num_labels=num_classes) for i in range(num_classes)}\n",
    "        self.class_wise_val_acc = {i: torchmetrics.Accuracy(task=\"multilabel\", num_labels=num_classes) for i in range(num_classes)}\n",
    "        self.class_wise_test_acc = {i: torchmetrics.Accuracy(task=\"multilabel\", num_labels=num_classes) for i in range(num_classes)}\n",
    "        # Precision Metrics\n",
    "        self.train_precision = MultilabelPrecision(num_labels=num_classes, average=\"none\")\n",
    "        self.val_precision = MultilabelPrecision(num_labels=num_classes, average=\"none\")\n",
    "        self.test_precision = MultilabelPrecision(num_labels=num_classes, average=\"none\")\n",
    "        # Recall Metrics\n",
    "        self.train_recall = MultilabelRecall(num_labels=num_classes, average=\"none\")\n",
    "        self.val_recall = MultilabelRecall(num_labels=num_classes, average=\"none\")\n",
    "        self.test_recall = MultilabelRecall(num_labels=num_classes, average=\"none\")\n",
    "\n",
    "        # F1 Metrics\n",
    "        self.train_f1 = MultilabelF1Score(num_labels=num_classes, average=\"none\")\n",
    "        self.val_f1 = MultilabelF1Score(num_labels=num_classes, average=\"none\")\n",
    "        self.test_f1 = MultilabelF1Score(num_labels=num_classes, average=\"none\")\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _shared_step(self, batch):\n",
    "        input_ids = batch['input_ids']  # Tokenized input\n",
    "        true_labels = batch['labels']  # Multi-hot encoded labels\n",
    "\n",
    "        logits = self.model(input_ids)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = F.cross_entropy(logits, true_labels)\n",
    "        \n",
    "        # Compute the predicted labels by applying a threshold\n",
    "        probabilities = F.softmax(logits, dim=-1) \n",
    "\n",
    "        predicted_labels_idx = torch.argmax(probabilities, dim=-1)\n",
    "        \n",
    "        batch_size = predicted_labels_idx.size(0)\n",
    "        num_classes = probabilities.size(1)  # Number of classes\n",
    "        one_hot_predictions = torch.zeros(batch_size, num_classes, device=logits.device)\n",
    "        one_hot_predictions.scatter_(1, predicted_labels_idx.unsqueeze(1), 1)\n",
    "        \n",
    "        return loss, true_labels, one_hot_predictions\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.train_acc(predicted_labels, true_labels)\n",
    "        self.log(\"train_acc\", self.train_acc, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        # Class-wise accuracy logging\n",
    "        class_accuracies = self._calculate_class_accuracy(predicted_labels, true_labels)\n",
    "\n",
    "        precision_values = self.train_precision(predicted_labels, true_labels)\n",
    "        for i, precision in enumerate(precision_values):\n",
    "            self.log(f\"train_precision_class_{i}\", precision, prog_bar=True)\n",
    "\n",
    "        # Recall Logging\n",
    "        recall_values = self.train_recall(predicted_labels, true_labels)\n",
    "        for i, recall in enumerate(recall_values):\n",
    "            self.log(f\"train_recall_class_{i}\", recall, prog_bar=True)\n",
    "\n",
    "        # F1 Logging\n",
    "        f1_values = self.train_f1(predicted_labels, true_labels)\n",
    "        for i, f1 in enumerate(f1_values):\n",
    "            self.log(f\"train_f1_class_{i}\", f1, prog_bar=True)\n",
    "\n",
    "        # for class_idx, accuracy in class_accuracies.items():\n",
    "        #     self.log(f\"class_{class_idx}_train_acc\", accuracy, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.val_acc(predicted_labels, true_labels)\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar=True)\n",
    "        # Class-wise accuracy logging\n",
    "        class_accuracies = self._calculate_class_accuracy(predicted_labels, true_labels)\n",
    "\n",
    "        # for class_idx, accuracy in class_accuracies.items():\n",
    "        #     self.log(f\"class_{class_idx}_val_acc\", accuracy, prog_bar=True)\n",
    "\n",
    "        # Precision\n",
    "        precision_values = self.val_precision(predicted_labels, true_labels)\n",
    "        for i, precision in enumerate(precision_values):\n",
    "            self.log(f\"val_precision_class_{i}\", precision, prog_bar=True)\n",
    "\n",
    "        # Recall Logging\n",
    "        recall_values = self.val_recall(predicted_labels, true_labels)\n",
    "        for i, recall in enumerate(recall_values):\n",
    "            self.log(f\"val_recall_class_{i}\", recall, prog_bar=True)\n",
    "\n",
    "        # F1 Logging\n",
    "        f1_values = self.val_f1(predicted_labels, true_labels)\n",
    "        for i, f1 in enumerate(f1_values):\n",
    "            self.log(f\"val_f1_class_{i}\", f1, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "        self.test_acc(predicted_labels, true_labels)\n",
    "        self.log(\"test_acc\", self.test_acc)\n",
    "\n",
    "        class_accuracies = self._calculate_class_accuracy(predicted_labels, true_labels)\n",
    "        \n",
    "        # for class_idx, accuracy in class_accuracies.items():\n",
    "        #     self.log(f\"class_{class_idx}_test_acc\", accuracy)\n",
    "        # Precision\n",
    "        precision_values = self.test_precision(predicted_labels, true_labels)\n",
    "        for i, precision in enumerate(precision_values):\n",
    "            self.log(f\"test_precision_class_{i}\", precision)\n",
    "\n",
    "        # Recall Logging\n",
    "        recall_values = self.test_recall(predicted_labels, true_labels)\n",
    "        for i, recall in enumerate(recall_values):\n",
    "            self.log(f\"test_recall_class_{i}\", recall)\n",
    "\n",
    "        # F1 Logging\n",
    "        f1_values = self.test_f1(predicted_labels, true_labels)\n",
    "        for i, f1 in enumerate(f1_values):\n",
    "            self.log(f\"test_f1_class_{i}\", f1)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def _calculate_class_accuracy(self, predicted_labels, true_labels):\n",
    "        class_accuracies = {}\n",
    "\n",
    "        # Compute correct predictions per class\n",
    "        correct_per_class = (predicted_labels * true_labels).sum(dim=0)  # Element-wise AND followed by sum across batch\n",
    "        total_per_class = true_labels.sum(dim=0)  # Total true instances per class\n",
    "\n",
    "        # Calculate accuracy for each class, avoiding division by zero\n",
    "        for i in range(self.num_classes):\n",
    "            correct = correct_per_class[i].item()\n",
    "            total = total_per_class[i].item()\n",
    "\n",
    "            if total > 0:\n",
    "                class_accuracies[i] = correct / total\n",
    "            else:\n",
    "                class_accuracies[i] = 0.0 \n",
    "\n",
    "        return class_accuracies\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(123)\n",
    "vocab_size = 30522  #For BERT tokenizer\n",
    "embed_dim = 768     #embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name            | Type                | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0  | model           | PyTorchCNN          | 24.6 M | train\n",
      "1  | train_acc       | MultilabelAccuracy  | 0      | train\n",
      "2  | val_acc         | MultilabelAccuracy  | 0      | train\n",
      "3  | test_acc        | MultilabelAccuracy  | 0      | train\n",
      "4  | train_precision | MultilabelPrecision | 0      | train\n",
      "5  | val_precision   | MultilabelPrecision | 0      | train\n",
      "6  | test_precision  | MultilabelPrecision | 0      | train\n",
      "7  | train_recall    | MultilabelRecall    | 0      | train\n",
      "8  | val_recall      | MultilabelRecall    | 0      | train\n",
      "9  | test_recall     | MultilabelRecall    | 0      | train\n",
      "10 | train_f1        | MultilabelF1Score   | 0      | train\n",
      "11 | val_f1          | MultilabelF1Score   | 0      | train\n",
      "12 | test_f1         | MultilabelF1Score   | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "24.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.6 M    Total params\n",
      "98.550    Total estimated model params size (MB)\n",
      "38        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kingk\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kingk\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 191/191 [02:08<00:00,  1.49it/s, v_num=75, train_precision_class_0=1.000, train_precision_class_1=0.000, train_precision_class_2=0.000, train_precision_class_3=0.250, train_precision_class_4=0.000, train_precision_class_5=0.000, train_precision_class_6=0.000, train_recall_class_0=0.625, train_recall_class_1=0.000, train_recall_class_2=0.000, train_recall_class_3=0.333, train_recall_class_4=0.000, train_recall_class_5=0.000, train_recall_class_6=0.000, train_f1_class_0=0.769, train_f1_class_1=0.000, train_f1_class_2=0.000, train_f1_class_3=0.286, train_f1_class_4=0.000, train_f1_class_5=0.000, train_f1_class_6=0.000, val_loss=1.190, val_acc=0.871, val_precision_class_0=0.643, val_precision_class_1=0.042, val_precision_class_2=0.084, val_precision_class_3=0.285, val_precision_class_4=0.418, val_precision_class_5=0.000, val_precision_class_6=0.000, val_recall_class_0=0.940, val_recall_class_1=0.0315, val_recall_class_2=0.0375, val_recall_class_3=0.061, val_recall_class_4=0.823, val_recall_class_5=0.000, val_recall_class_6=0.000, val_f1_class_0=0.759, val_f1_class_1=0.035, val_f1_class_2=0.0469, val_f1_class_3=0.0954, val_f1_class_4=0.530, val_f1_class_5=0.000, val_f1_class_6=0.000, train_acc=0.860]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 191/191 [02:11<00:00,  1.45it/s, v_num=75, train_precision_class_0=1.000, train_precision_class_1=0.000, train_precision_class_2=0.000, train_precision_class_3=0.000, train_precision_class_4=0.000, train_precision_class_5=0.000, train_precision_class_6=0.000, train_recall_class_0=0.667, train_recall_class_1=0.000, train_recall_class_2=0.000, train_recall_class_3=0.000, train_recall_class_4=0.000, train_recall_class_5=0.000, train_recall_class_6=0.000, train_f1_class_0=0.800, train_f1_class_1=0.000, train_f1_class_2=0.000, train_f1_class_3=0.000, train_f1_class_4=0.000, train_f1_class_5=0.000, train_f1_class_6=0.000, val_loss=0.925, val_acc=0.928, val_precision_class_0=0.741, val_precision_class_1=0.000, val_precision_class_2=0.021, val_precision_class_3=0.920, val_precision_class_4=0.634, val_precision_class_5=0.000, val_precision_class_6=0.000, val_recall_class_0=0.990, val_recall_class_1=0.000, val_recall_class_2=0.00525, val_recall_class_3=0.722, val_recall_class_4=0.776, val_recall_class_5=0.000, val_recall_class_6=0.000, val_f1_class_0=0.844, val_f1_class_1=0.000, val_f1_class_2=0.0084, val_f1_class_3=0.801, val_f1_class_4=0.676, val_f1_class_5=0.000, val_f1_class_6=0.000, train_acc=0.928] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.266 >= min_delta = 0.0. New best score: 0.925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 191/191 [02:12<00:00,  1.45it/s, v_num=75, train_precision_class_0=0.833, train_precision_class_1=0.000, train_precision_class_2=0.000, train_precision_class_3=1.000, train_precision_class_4=0.333, train_precision_class_5=0.000, train_precision_class_6=0.000, train_recall_class_0=1.000, train_recall_class_1=0.000, train_recall_class_2=0.000, train_recall_class_3=1.000, train_recall_class_4=0.500, train_recall_class_5=0.000, train_recall_class_6=0.000, train_f1_class_0=0.909, train_f1_class_1=0.000, train_f1_class_2=0.000, train_f1_class_3=1.000, train_f1_class_4=0.400, train_f1_class_5=0.000, train_f1_class_6=0.000, val_loss=0.628, val_acc=0.947, val_precision_class_0=0.863, val_precision_class_1=0.336, val_precision_class_2=0.404, val_precision_class_3=0.881, val_precision_class_4=0.754, val_precision_class_5=0.000, val_precision_class_6=0.000, val_recall_class_0=0.953, val_recall_class_1=0.288, val_recall_class_2=0.325, val_recall_class_3=0.871, val_recall_class_4=0.785, val_recall_class_5=0.000, val_recall_class_6=0.000, val_f1_class_0=0.904, val_f1_class_1=0.301, val_f1_class_2=0.348, val_f1_class_3=0.871, val_f1_class_4=0.746, val_f1_class_5=0.000, val_f1_class_6=0.000, train_acc=0.941]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.297 >= min_delta = 0.0. New best score: 0.628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 191/191 [02:11<00:00,  1.45it/s, v_num=75, train_precision_class_0=0.833, train_precision_class_1=0.000, train_precision_class_2=1.000, train_precision_class_3=1.000, train_precision_class_4=1.000, train_precision_class_5=0.000, train_precision_class_6=0.000, train_recall_class_0=1.000, train_recall_class_1=0.000, train_recall_class_2=0.500, train_recall_class_3=1.000, train_recall_class_4=1.000, train_recall_class_5=0.000, train_recall_class_6=0.000, train_f1_class_0=0.909, train_f1_class_1=0.000, train_f1_class_2=0.667, train_f1_class_3=1.000, train_f1_class_4=1.000, train_f1_class_5=0.000, train_f1_class_6=0.000, val_loss=0.608, val_acc=0.951, val_precision_class_0=0.895, val_precision_class_1=0.625, val_precision_class_2=0.417, val_precision_class_3=0.930, val_precision_class_4=0.788, val_precision_class_5=0.000, val_precision_class_6=0.182, val_recall_class_0=0.927, val_recall_class_1=0.674, val_recall_class_2=0.343, val_recall_class_3=0.855, val_recall_class_4=0.794, val_recall_class_5=0.000, val_recall_class_6=0.166, val_f1_class_0=0.908, val_f1_class_1=0.640, val_f1_class_2=0.358, val_f1_class_3=0.885, val_f1_class_4=0.771, val_f1_class_5=0.000, val_f1_class_6=0.157, train_acc=0.963]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.020 >= min_delta = 0.0. New best score: 0.608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 191/191 [02:11<00:00,  1.45it/s, v_num=75, train_precision_class_0=1.000, train_precision_class_1=0.000, train_precision_class_2=1.000, train_precision_class_3=1.000, train_precision_class_4=1.000, train_precision_class_5=0.000, train_precision_class_6=0.000, train_recall_class_0=1.000, train_recall_class_1=0.000, train_recall_class_2=1.000, train_recall_class_3=1.000, train_recall_class_4=1.000, train_recall_class_5=0.000, train_recall_class_6=0.000, train_f1_class_0=1.000, train_f1_class_1=0.000, train_f1_class_2=1.000, train_f1_class_3=1.000, train_f1_class_4=1.000, train_f1_class_5=0.000, train_f1_class_6=0.000, val_loss=0.703, val_acc=0.948, val_precision_class_0=0.931, val_precision_class_1=0.643, val_precision_class_2=0.389, val_precision_class_3=0.919, val_precision_class_4=0.863, val_precision_class_5=0.202, val_precision_class_6=0.268, val_recall_class_0=0.835, val_recall_class_1=0.639, val_recall_class_2=0.389, val_recall_class_3=0.881, val_recall_class_4=0.815, val_recall_class_5=0.170, val_recall_class_6=0.382, val_f1_class_0=0.878, val_f1_class_1=0.637, val_f1_class_2=0.367, val_f1_class_3=0.896, val_f1_class_4=0.813, val_f1_class_5=0.181, val_f1_class_6=0.297, train_acc=0.989]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 10 records. Best score: 0.608. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 191/191 [02:11<00:00,  1.45it/s, v_num=75, train_precision_class_0=1.000, train_precision_class_1=0.000, train_precision_class_2=1.000, train_precision_class_3=1.000, train_precision_class_4=1.000, train_precision_class_5=0.000, train_precision_class_6=0.000, train_recall_class_0=1.000, train_recall_class_1=0.000, train_recall_class_2=1.000, train_recall_class_3=1.000, train_recall_class_4=1.000, train_recall_class_5=0.000, train_recall_class_6=0.000, train_f1_class_0=1.000, train_f1_class_1=0.000, train_f1_class_2=1.000, train_f1_class_3=1.000, train_f1_class_4=1.000, train_f1_class_5=0.000, train_f1_class_6=0.000, val_loss=0.703, val_acc=0.948, val_precision_class_0=0.931, val_precision_class_1=0.643, val_precision_class_2=0.389, val_precision_class_3=0.919, val_precision_class_4=0.863, val_precision_class_5=0.202, val_precision_class_6=0.268, val_recall_class_0=0.835, val_recall_class_1=0.639, val_recall_class_2=0.389, val_recall_class_3=0.881, val_recall_class_4=0.815, val_recall_class_5=0.170, val_recall_class_6=0.382, val_f1_class_0=0.878, val_f1_class_1=0.637, val_f1_class_2=0.367, val_f1_class_3=0.896, val_f1_class_4=0.813, val_f1_class_5=0.181, val_f1_class_6=0.297, train_acc=0.989]\n"
     ]
    }
   ],
   "source": [
    "pytorch_model = PyTorchCNN(vocab_size=vocab_size, embed_dim=embed_dim,num_classes=7)  \n",
    "lightning_model = LightningModel(model=pytorch_model, learning_rate=0.01,num_classes=7 )\n",
    "\n",
    "# Setup PyTorch Lightning trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=200,\n",
    "    accelerator=\"gpu\",  # Change to \"gpu\" if you want to use a GPU\n",
    "    devices=\"auto\",\n",
    "    callbacks=[early_stopping],\n",
    "    logger=CSVLogger(save_dir=\"logs/\", name=\"my-model\"),\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(lightning_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\kingk\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 54/54 [00:02<00:00, 21.09it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9122152924537659\n",
      "     test_f1_class_0        0.6910209059715271\n",
      "     test_f1_class_1        0.49542027711868286\n",
      "     test_f1_class_2        0.09800131618976593\n",
      "     test_f1_class_3        0.6750164031982422\n",
      "     test_f1_class_4        0.5982286930084229\n",
      "     test_f1_class_5        0.10512077808380127\n",
      "     test_f1_class_6        0.1029297485947609\n",
      " test_precision_class_0     0.8559073805809021\n",
      " test_precision_class_1     0.4991304278373718\n",
      " test_precision_class_2     0.10679098218679428\n",
      " test_precision_class_3     0.6961787939071655\n",
      " test_precision_class_4     0.6044206619262695\n",
      " test_precision_class_5     0.11130435019731522\n",
      " test_precision_class_6     0.09328364580869675\n",
      "   test_recall_class_0      0.6058998107910156\n",
      "   test_recall_class_1      0.49294689297676086\n",
      "   test_recall_class_2      0.10322607308626175\n",
      "   test_recall_class_3      0.7043520212173462\n",
      "   test_recall_class_4      0.6157073974609375\n",
      "   test_recall_class_5       0.102028988301754\n",
      "   test_recall_class_6      0.15026088058948517\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.9122152924537659,\n",
       "  'test_precision_class_0': 0.8559073805809021,\n",
       "  'test_precision_class_1': 0.4991304278373718,\n",
       "  'test_precision_class_2': 0.10679098218679428,\n",
       "  'test_precision_class_3': 0.6961787939071655,\n",
       "  'test_precision_class_4': 0.6044206619262695,\n",
       "  'test_precision_class_5': 0.11130435019731522,\n",
       "  'test_precision_class_6': 0.09328364580869675,\n",
       "  'test_recall_class_0': 0.6058998107910156,\n",
       "  'test_recall_class_1': 0.49294689297676086,\n",
       "  'test_recall_class_2': 0.10322607308626175,\n",
       "  'test_recall_class_3': 0.7043520212173462,\n",
       "  'test_recall_class_4': 0.6157073974609375,\n",
       "  'test_recall_class_5': 0.102028988301754,\n",
       "  'test_recall_class_6': 0.15026088058948517,\n",
       "  'test_f1_class_0': 0.6910209059715271,\n",
       "  'test_f1_class_1': 0.49542027711868286,\n",
       "  'test_f1_class_2': 0.09800131618976593,\n",
       "  'test_f1_class_3': 0.6750164031982422,\n",
       "  'test_f1_class_4': 0.5982286930084229,\n",
       "  'test_f1_class_5': 0.10512077808380127,\n",
       "  'test_f1_class_6': 0.1029297485947609}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=lightning_model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Modify Dataset Class for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PythonCommentDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.comments = dataframe['combo'].tolist()\n",
    "        self.labels = dataframe['labels'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the text\n",
    "        text = self.comments[idx]\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Process labels\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        \n",
    "        # Reshape input for CNN\n",
    "        input_ids = tokens['input_ids'].squeeze(0)\n",
    "        \n",
    "        # Reshape embeddings to match CNN input format [batch_size, channels, sequence_length, embedding_dim]\n",
    "        cnn_input = input_ids.unsqueeze(0)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': cnn_input,\n",
    "            'labels': label\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load Python Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-set tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "max_len = 512 \n",
    "\n",
    "# Prepare Dataset\n",
    "python_train_dataset = PythonCommentDataset(python_train_data, tokenizer, max_len)\n",
    "python_val_dataset = PythonCommentDataset(python_val_data, tokenizer, max_len)\n",
    "python_test_dataset = PythonCommentDataset(python_test, tokenizer, max_len)\n",
    "\n",
    "# Dataloaders\n",
    "python_train_loader = DataLoader(python_train_dataset, batch_size=32, shuffle=True)\n",
    "python_val_loader = DataLoader(python_val_dataset, batch_size=32, shuffle=False)\n",
    "python_test_loader = DataLoader(python_test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Update Models for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model for Python comments\n",
    "pytorch_python_model = PyTorchCNN(vocab_size=vocab_size, embed_dim=embed_dim, num_classes=5)\n",
    "lightning_python_model = LightningModel(model=pytorch_python_model, learning_rate=0.01, num_classes=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Train model for Python comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Setup PyTorch Lightning trainer for CPU\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=200,\n",
    "    accelerator=\"gpu\",  # Use CPU instead of GPU\n",
    "    devices=\"auto\",\n",
    "    callbacks=[early_stopping],  \n",
    "    logger=CSVLogger(save_dir=\"logs/\", name=\"my-model\"),\n",
    "    deterministic=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name            | Type                | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0  | model           | PyTorchCNN          | 24.6 M | train\n",
      "1  | train_acc       | MultilabelAccuracy  | 0      | train\n",
      "2  | val_acc         | MultilabelAccuracy  | 0      | train\n",
      "3  | test_acc        | MultilabelAccuracy  | 0      | train\n",
      "4  | train_precision | MultilabelPrecision | 0      | train\n",
      "5  | val_precision   | MultilabelPrecision | 0      | train\n",
      "6  | test_precision  | MultilabelPrecision | 0      | train\n",
      "7  | train_recall    | MultilabelRecall    | 0      | train\n",
      "8  | val_recall      | MultilabelRecall    | 0      | train\n",
      "9  | test_recall     | MultilabelRecall    | 0      | train\n",
      "10 | train_f1        | MultilabelF1Score   | 0      | train\n",
      "11 | val_f1          | MultilabelF1Score   | 0      | train\n",
      "12 | test_f1         | MultilabelF1Score   | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "24.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.6 M    Total params\n",
      "98.549    Total estimated model params size (MB)\n",
      "38        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kingk\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kingk\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\kingk\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (48) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 48/48 [00:31<00:00,  1.51it/s, v_num=77, train_precision_class_0=0.000, train_precision_class_1=0.000, train_precision_class_2=0.000, train_precision_class_3=0.000, train_precision_class_4=0.000, train_recall_class_0=0.000, train_recall_class_1=0.000, train_recall_class_2=0.000, train_recall_class_3=0.000, train_recall_class_4=0.000, train_f1_class_0=0.000, train_f1_class_1=0.000, train_f1_class_2=0.000, train_f1_class_3=0.000, train_f1_class_4=0.000, val_loss=1.630, val_acc=0.736, val_precision_class_0=0.530, val_precision_class_1=0.344, val_precision_class_2=0.0849, val_precision_class_3=0.000, val_precision_class_4=0.271, val_recall_class_0=0.459, val_recall_class_1=0.546, val_recall_class_2=0.0283, val_recall_class_3=0.000, val_recall_class_4=0.379, val_f1_class_0=0.486, val_f1_class_1=0.412, val_f1_class_2=0.0424, val_f1_class_3=0.000, val_f1_class_4=0.289, train_acc=0.710]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 48/48 [00:33<00:00,  1.41it/s, v_num=77, train_precision_class_0=1.000, train_precision_class_1=0.500, train_precision_class_2=0.000, train_precision_class_3=0.000, train_precision_class_4=0.000, train_recall_class_0=1.000, train_recall_class_1=1.000, train_recall_class_2=0.000, train_recall_class_3=0.000, train_recall_class_4=0.000, train_f1_class_0=1.000, train_f1_class_1=0.667, train_f1_class_2=0.000, train_f1_class_3=0.000, train_f1_class_4=0.000, val_loss=1.430, val_acc=0.785, val_precision_class_0=0.666, val_precision_class_1=0.571, val_precision_class_2=0.000, val_precision_class_3=0.153, val_precision_class_4=0.354, val_recall_class_0=0.708, val_recall_class_1=0.641, val_recall_class_2=0.000, val_recall_class_3=0.124, val_recall_class_4=0.313, val_f1_class_0=0.678, val_f1_class_1=0.593, val_f1_class_2=0.000, val_f1_class_3=0.125, val_f1_class_4=0.308, train_acc=0.801]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.202 >= min_delta = 0.0. New best score: 1.427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 48/48 [00:33<00:00,  1.42it/s, v_num=77, train_precision_class_0=0.000, train_precision_class_1=1.000, train_precision_class_2=0.000, train_precision_class_3=0.000, train_precision_class_4=0.000, train_recall_class_0=0.000, train_recall_class_1=0.333, train_recall_class_2=0.000, train_recall_class_3=0.000, train_recall_class_4=0.000, train_f1_class_0=0.000, train_f1_class_1=0.500, train_f1_class_2=0.000, train_f1_class_3=0.000, train_f1_class_4=0.000, val_loss=1.670, val_acc=0.806, val_precision_class_0=0.793, val_precision_class_1=0.698, val_precision_class_2=0.198, val_precision_class_3=0.340, val_precision_class_4=0.407, val_recall_class_0=0.646, val_recall_class_1=0.551, val_recall_class_2=0.171, val_recall_class_3=0.324, val_recall_class_4=0.481, val_f1_class_0=0.703, val_f1_class_1=0.602, val_f1_class_2=0.181, val_f1_class_3=0.311, val_f1_class_4=0.433, train_acc=0.894] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 10 records. Best score: 1.427. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 48/48 [00:34<00:00,  1.39it/s, v_num=77, train_precision_class_0=0.000, train_precision_class_1=1.000, train_precision_class_2=0.000, train_precision_class_3=0.000, train_precision_class_4=0.000, train_recall_class_0=0.000, train_recall_class_1=0.333, train_recall_class_2=0.000, train_recall_class_3=0.000, train_recall_class_4=0.000, train_f1_class_0=0.000, train_f1_class_1=0.500, train_f1_class_2=0.000, train_f1_class_3=0.000, train_f1_class_4=0.000, val_loss=1.670, val_acc=0.806, val_precision_class_0=0.793, val_precision_class_1=0.698, val_precision_class_2=0.198, val_precision_class_3=0.340, val_precision_class_4=0.407, val_recall_class_0=0.646, val_recall_class_1=0.551, val_recall_class_2=0.171, val_recall_class_3=0.324, val_recall_class_4=0.481, val_f1_class_0=0.703, val_f1_class_1=0.602, val_f1_class_2=0.181, val_f1_class_3=0.311, val_f1_class_4=0.433, train_acc=0.894]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\kingk\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 13/13 [00:00<00:00, 24.25it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.7674877047538757\n",
      "     test_f1_class_0        0.3780786395072937\n",
      "     test_f1_class_1        0.4347551167011261\n",
      "     test_f1_class_2        0.15749779343605042\n",
      "     test_f1_class_3        0.2125399112701416\n",
      "     test_f1_class_4        0.3562993109226227\n",
      " test_precision_class_0     0.5348981022834778\n",
      " test_precision_class_1     0.6206597685813904\n",
      " test_precision_class_2     0.2048635482788086\n",
      " test_precision_class_3     0.1990695297718048\n",
      " test_precision_class_4     0.3929506242275238\n",
      "   test_recall_class_0      0.3130086064338684\n",
      "   test_recall_class_1      0.38730278611183167\n",
      "   test_recall_class_2      0.1707717627286911\n",
      "   test_recall_class_3      0.29173511266708374\n",
      "   test_recall_class_4      0.36346864700317383\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.7674877047538757,\n",
       "  'test_precision_class_0': 0.5348981022834778,\n",
       "  'test_precision_class_1': 0.6206597685813904,\n",
       "  'test_precision_class_2': 0.2048635482788086,\n",
       "  'test_precision_class_3': 0.1990695297718048,\n",
       "  'test_precision_class_4': 0.3929506242275238,\n",
       "  'test_recall_class_0': 0.3130086064338684,\n",
       "  'test_recall_class_1': 0.38730278611183167,\n",
       "  'test_recall_class_2': 0.1707717627286911,\n",
       "  'test_recall_class_3': 0.29173511266708374,\n",
       "  'test_recall_class_4': 0.36346864700317383,\n",
       "  'test_f1_class_0': 0.3780786395072937,\n",
       "  'test_f1_class_1': 0.4347551167011261,\n",
       "  'test_f1_class_2': 0.15749779343605042,\n",
       "  'test_f1_class_3': 0.2125399112701416,\n",
       "  'test_f1_class_4': 0.3562993109226227}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train the model on Python dataset\n",
    "trainer.fit(lightning_python_model, train_dataloaders=python_train_loader, val_dataloaders=python_val_loader)\n",
    "\n",
    "# Test the model on Python test dataset\n",
    "trainer.test(model=lightning_python_model, dataloaders=python_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pharo Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Setup Dataset for Pharo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pharo dataset follows the java dataset very closely as it contains the same number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PharoCommentDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.comments = dataframe['combo'].tolist()\n",
    "        self.labels = dataframe['labels'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the text\n",
    "        text = self.comments[idx]\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Process labels\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        \n",
    "        # Reshape input for CNN\n",
    "        input_ids = tokens['input_ids'].squeeze(0)\n",
    "        \n",
    "        # Reshape embeddings to match CNN input format [batch_size, channels, sequence_length, embedding_dim]\n",
    "        cnn_input = input_ids.unsqueeze(0)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': cnn_input,\n",
    "            'labels': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pharo dataset follows the java dataset very closely as it contains the same number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Reset model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "max_len = 512 \n",
    "\n",
    "# Prepare Dataset for the Pharo comments\n",
    "pharo_train_dataset = PharoCommentDataset(pharo_train_data, tokenizer, max_len)\n",
    "pharo_train_loader = DataLoader(pharo_train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "pharo_val_dataset = PharoCommentDataset(pharo_val_data, tokenizer, max_len)\n",
    "pharo_val_loader = DataLoader(pharo_val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "pharo_test_dataset = PharoCommentDataset(pharo_test, tokenizer, max_len)\n",
    "pharo_test_loader = DataLoader(pharo_test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Implement models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model for Pharo comments (7 classes)\n",
    "pytorch_pharo_model = PyTorchCNN(vocab_size=vocab_size, embed_dim=embed_dim, num_classes=7)\n",
    "lightning_pharo_model = LightningModel(model=pytorch_pharo_model, learning_rate=0.01, num_classes=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Set for Pharo comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Setup PyTorch Lightning trainer for GPU\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=200,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=\"auto\",\n",
    "    callbacks=[early_stopping],   \n",
    "    logger=CSVLogger(save_dir=\"logs/\", name=\"my-model\"),\n",
    "    deterministic=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name            | Type                | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0  | model           | PyTorchCNN          | 24.6 M | train\n",
      "1  | train_acc       | MultilabelAccuracy  | 0      | train\n",
      "2  | val_acc         | MultilabelAccuracy  | 0      | train\n",
      "3  | test_acc        | MultilabelAccuracy  | 0      | train\n",
      "4  | train_precision | MultilabelPrecision | 0      | train\n",
      "5  | val_precision   | MultilabelPrecision | 0      | train\n",
      "6  | test_precision  | MultilabelPrecision | 0      | train\n",
      "7  | train_recall    | MultilabelRecall    | 0      | train\n",
      "8  | val_recall      | MultilabelRecall    | 0      | train\n",
      "9  | test_recall     | MultilabelRecall    | 0      | train\n",
      "10 | train_f1        | MultilabelF1Score   | 0      | train\n",
      "11 | val_f1          | MultilabelF1Score   | 0      | train\n",
      "12 | test_f1         | MultilabelF1Score   | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "24.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.6 M    Total params\n",
      "98.550    Total estimated model params size (MB)\n",
      "38        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kingk\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kingk\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\kingk\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (33) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 33/33 [00:24<00:00,  1.35it/s, v_num=78, train_precision_class_0=0.000, train_precision_class_1=0.500, train_precision_class_2=0.600, train_precision_class_3=0.000, train_precision_class_4=0.000, train_precision_class_5=0.000, train_precision_class_6=0.000, train_recall_class_0=0.000, train_recall_class_1=0.375, train_recall_class_2=0.750, train_recall_class_3=0.000, train_recall_class_4=0.000, train_recall_class_5=0.000, train_recall_class_6=0.000, train_f1_class_0=0.000, train_f1_class_1=0.429, train_f1_class_2=0.667, train_f1_class_3=0.000, train_f1_class_4=0.000, train_f1_class_5=0.000, train_f1_class_6=0.000, val_loss=1.820, val_acc=0.807, val_precision_class_0=0.238, val_precision_class_1=0.571, val_precision_class_2=0.326, val_precision_class_3=0.000, val_precision_class_4=0.000, val_precision_class_5=0.444, val_precision_class_6=0.000, val_recall_class_0=0.239, val_recall_class_1=0.474, val_recall_class_2=0.470, val_recall_class_3=0.000, val_recall_class_4=0.000, val_recall_class_5=0.293, val_recall_class_6=0.000, val_f1_class_0=0.234, val_f1_class_1=0.505, val_f1_class_2=0.364, val_f1_class_3=0.000, val_f1_class_4=0.000, val_f1_class_5=0.320, val_f1_class_6=0.000, train_acc=0.812]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 33/33 [00:24<00:00,  1.32it/s, v_num=78, train_precision_class_0=0.500, train_precision_class_1=0.571, train_precision_class_2=0.500, train_precision_class_3=0.000, train_precision_class_4=0.000, train_precision_class_5=0.000, train_precision_class_6=0.000, train_recall_class_0=0.200, train_recall_class_1=1.000, train_recall_class_2=1.000, train_recall_class_3=0.000, train_recall_class_4=0.000, train_recall_class_5=0.000, train_recall_class_6=0.000, train_f1_class_0=0.286, train_f1_class_1=0.727, train_f1_class_2=0.667, train_f1_class_3=0.000, train_f1_class_4=0.000, train_f1_class_5=0.000, train_f1_class_6=0.000, val_loss=1.510, val_acc=0.855, val_precision_class_0=0.482, val_precision_class_1=0.563, val_precision_class_2=0.596, val_precision_class_3=0.000, val_precision_class_4=0.123, val_precision_class_5=0.246, val_precision_class_6=0.000, val_recall_class_0=0.326, val_recall_class_1=0.930, val_recall_class_2=0.453, val_recall_class_3=0.000, val_recall_class_4=0.123, val_recall_class_5=0.0615, val_recall_class_6=0.000, val_f1_class_0=0.372, val_f1_class_1=0.692, val_f1_class_2=0.470, val_f1_class_3=0.000, val_f1_class_4=0.123, val_f1_class_5=0.0967, val_f1_class_6=0.000, train_acc=0.841]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.317 >= min_delta = 0.0. New best score: 1.507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 33/33 [00:20<00:00,  1.58it/s, v_num=78, train_precision_class_0=0.333, train_precision_class_1=0.600, train_precision_class_2=0.500, train_precision_class_3=0.000, train_precision_class_4=0.000, train_precision_class_5=1.000, train_precision_class_6=0.000, train_recall_class_0=1.000, train_recall_class_1=1.000, train_recall_class_2=0.500, train_recall_class_3=0.000, train_recall_class_4=0.000, train_recall_class_5=0.500, train_recall_class_6=0.000, train_f1_class_0=0.500, train_f1_class_1=0.750, train_f1_class_2=0.500, train_f1_class_3=0.000, train_f1_class_4=0.000, train_f1_class_5=0.667, train_f1_class_6=0.000, val_loss=1.260, val_acc=0.894, val_precision_class_0=0.727, val_precision_class_1=0.827, val_precision_class_2=0.473, val_precision_class_3=0.000, val_precision_class_4=0.519, val_precision_class_5=0.729, val_precision_class_6=0.000, val_recall_class_0=0.445, val_recall_class_1=0.847, val_recall_class_2=0.620, val_recall_class_3=0.000, val_recall_class_4=0.349, val_recall_class_5=0.536, val_recall_class_6=0.000, val_f1_class_0=0.543, val_f1_class_1=0.834, val_f1_class_2=0.510, val_f1_class_3=0.000, val_f1_class_4=0.361, val_f1_class_5=0.604, val_f1_class_6=0.000, train_acc=0.892]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.245 >= min_delta = 0.0. New best score: 1.262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 33/33 [00:20<00:00,  1.57it/s, v_num=78, train_precision_class_0=0.000, train_precision_class_1=1.000, train_precision_class_2=1.000, train_precision_class_3=0.000, train_precision_class_4=1.000, train_precision_class_5=1.000, train_precision_class_6=0.000, train_recall_class_0=0.000, train_recall_class_1=1.000, train_recall_class_2=0.400, train_recall_class_3=0.000, train_recall_class_4=1.000, train_recall_class_5=1.000, train_recall_class_6=0.000, train_f1_class_0=0.000, train_f1_class_1=1.000, train_f1_class_2=0.571, train_f1_class_3=0.000, train_f1_class_4=1.000, train_f1_class_5=1.000, train_f1_class_6=0.000, val_loss=1.480, val_acc=0.896, val_precision_class_0=0.536, val_precision_class_1=0.838, val_precision_class_2=0.618, val_precision_class_3=0.0615, val_precision_class_4=0.656, val_precision_class_5=0.606, val_precision_class_6=0.103, val_recall_class_0=0.552, val_recall_class_1=0.869, val_recall_class_2=0.337, val_recall_class_3=0.0308, val_recall_class_4=0.451, val_recall_class_5=0.686, val_recall_class_6=0.164, val_f1_class_0=0.522, val_f1_class_1=0.851, val_f1_class_2=0.423, val_f1_class_3=0.041, val_f1_class_4=0.467, val_f1_class_5=0.629, val_f1_class_6=0.111, train_acc=0.960]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 10 records. Best score: 1.262. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 33/33 [00:22<00:00,  1.49it/s, v_num=78, train_precision_class_0=0.000, train_precision_class_1=1.000, train_precision_class_2=1.000, train_precision_class_3=0.000, train_precision_class_4=1.000, train_precision_class_5=1.000, train_precision_class_6=0.000, train_recall_class_0=0.000, train_recall_class_1=1.000, train_recall_class_2=0.400, train_recall_class_3=0.000, train_recall_class_4=1.000, train_recall_class_5=1.000, train_recall_class_6=0.000, train_f1_class_0=0.000, train_f1_class_1=1.000, train_f1_class_2=0.571, train_f1_class_3=0.000, train_f1_class_4=1.000, train_f1_class_5=1.000, train_f1_class_6=0.000, val_loss=1.480, val_acc=0.896, val_precision_class_0=0.536, val_precision_class_1=0.838, val_precision_class_2=0.618, val_precision_class_3=0.0615, val_precision_class_4=0.656, val_precision_class_5=0.606, val_precision_class_6=0.103, val_recall_class_0=0.552, val_recall_class_1=0.869, val_recall_class_2=0.337, val_recall_class_3=0.0308, val_recall_class_4=0.451, val_recall_class_5=0.686, val_recall_class_6=0.164, val_f1_class_0=0.522, val_f1_class_1=0.851, val_f1_class_2=0.423, val_f1_class_3=0.041, val_f1_class_4=0.467, val_f1_class_5=0.629, val_f1_class_6=0.111, train_acc=0.960]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\kingk\\anaconda3\\envs\\pytorchenv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 10/10 [00:00<00:00, 27.77it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.8882847428321838\n",
      "     test_f1_class_0        0.3781798779964447\n",
      "     test_f1_class_1        0.8131514191627502\n",
      "     test_f1_class_2        0.38167139887809753\n",
      "     test_f1_class_3        0.07381776720285416\n",
      "     test_f1_class_4        0.5096940994262695\n",
      "     test_f1_class_5         0.232232928276062\n",
      "     test_f1_class_6        0.21618059277534485\n",
      " test_precision_class_0     0.4463667869567871\n",
      " test_precision_class_1     0.8587366342544556\n",
      " test_precision_class_2     0.5886966586112976\n",
      " test_precision_class_3     0.11072664707899094\n",
      " test_precision_class_4     0.5314878821372986\n",
      " test_precision_class_5     0.25221067667007446\n",
      " test_precision_class_6     0.1845444142818451\n",
      "   test_recall_class_0       0.384212851524353\n",
      "   test_recall_class_1      0.8153007626533508\n",
      "   test_recall_class_2      0.3250617980957031\n",
      "   test_recall_class_3      0.05536332353949547\n",
      "   test_recall_class_4      0.5093425512313843\n",
      "   test_recall_class_5      0.25836217403411865\n",
      "   test_recall_class_6      0.3321799337863922\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.8882847428321838,\n",
       "  'test_precision_class_0': 0.4463667869567871,\n",
       "  'test_precision_class_1': 0.8587366342544556,\n",
       "  'test_precision_class_2': 0.5886966586112976,\n",
       "  'test_precision_class_3': 0.11072664707899094,\n",
       "  'test_precision_class_4': 0.5314878821372986,\n",
       "  'test_precision_class_5': 0.25221067667007446,\n",
       "  'test_precision_class_6': 0.1845444142818451,\n",
       "  'test_recall_class_0': 0.384212851524353,\n",
       "  'test_recall_class_1': 0.8153007626533508,\n",
       "  'test_recall_class_2': 0.3250617980957031,\n",
       "  'test_recall_class_3': 0.05536332353949547,\n",
       "  'test_recall_class_4': 0.5093425512313843,\n",
       "  'test_recall_class_5': 0.25836217403411865,\n",
       "  'test_recall_class_6': 0.3321799337863922,\n",
       "  'test_f1_class_0': 0.3781798779964447,\n",
       "  'test_f1_class_1': 0.8131514191627502,\n",
       "  'test_f1_class_2': 0.38167139887809753,\n",
       "  'test_f1_class_3': 0.07381776720285416,\n",
       "  'test_f1_class_4': 0.5096940994262695,\n",
       "  'test_f1_class_5': 0.232232928276062,\n",
       "  'test_f1_class_6': 0.21618059277534485}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using the pharo dataset\n",
    "trainer.fit(lightning_pharo_model, train_dataloaders=pharo_train_loader, val_dataloaders=pharo_val_loader)\n",
    "\n",
    "# Test the model on Pharo test dataset\n",
    "trainer.test(model=lightning_pharo_model, dataloaders=pharo_test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
